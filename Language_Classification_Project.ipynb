{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "\n",
    "from helper_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All data is saved under the name train_sentences.(language abbreviation) or val_sentences.(language abbreviation)\n",
    "#The data is first parsed and read using readlines and then the data is passed onto the keyvalue in data_raw\n",
    "#The encoding might differ but it can be solved easily, I used utf8 as well as cp1252\n",
    "\n",
    "def open_file(filename, encoding):\n",
    "    with open(filename, 'r',encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw=dict ()\n",
    "\n",
    "data_raw['sk']=open_file('Data/Sentences/train_sentences.sk', encoding=\"utf8\")\n",
    "data_raw['cs']=open_file('Data/Sentences/train_sentences.csz', encoding=\"utf8\")\n",
    "data_raw['en']=open_file('Data/Sentences/train_sentences.en', encoding=\"utf8\")\n",
    "\n",
    "#Here the encoding changed, so iused the open function and directly apploed readlines\n",
    "data_raw['fr']= open('Data/Sentences/train_sentences.fr',encoding='cp1252').readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to show the statistics of our data set for each language\n",
    "def show_statistics(data):\n",
    "    for language, sentences in data.items():\n",
    "        #the values saved inside each language such as sk, cs and en are then split using whitespace\n",
    "        word_list = ' '.join(sentences).split()\n",
    "        number_of_sentences = len(sentences)\n",
    "        number_of_words = len(word_list)\n",
    "        #unique words list is calculated using sets as sets dont repeat their contents\n",
    "        number_of_unique_words = len(set(word_list))\n",
    "        #sample extract , just to show what is present in the data as we move forward\n",
    "        sample_extract = ''.join(sentences[0].split()[0:7])\n",
    "        \n",
    "       \n",
    "        print(f'Language: {language}')\n",
    "        print('-----------------------')\n",
    "        print(f'Number of sentences\\t:\\t {number_of_sentences}')\n",
    "        print(f'Number of words\\t\\t:\\t {number_of_words}')\n",
    "        print(f'Number of unique words\\t:\\t {number_of_unique_words}')\n",
    "        print(f'Sample extract\\t\\t:\\t {sample_extract}...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_statistics(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''\n",
    "    Removes punctuation and digits from a string, and converts all characters to lowercase. \n",
    "    Also clears all \\n and hyphens (splits hyphenated words into two words).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    preprocessed_text = text\n",
    "    #.replace is used extensively to remove unwanted chars in the data\n",
    "    preprocessed_text=text.lower().replace('-',' ')\n",
    "    #import string done for string.punctuation\n",
    "    translation_table=str.maketrans('\\n',' ',string.punctuation+string.digits)\n",
    "    preprocessed_text=preprocessed_text.translate(translation_table)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function is run and saved as dictionary \n",
    "data_preprocessed={k:[preprocess(sentence)for sentence in v] for k,v in data_raw.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' Theorem**\n",
    "\n",
    "\\begin{equation}\n",
    "P(A | B)=\\frac{P(B | A) \\times P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's translate this theory into our specific problem. In our case, where we want to categorise a sentence `my name is Abhi` into one of `sk`, `cs`, or `en`, the following are the probabilities we want to determine.\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text {sk} | \\text {my name is Abhi})=\\frac{P(\\text {my name is Abhi} | \\text {sk}) \\times P(\\text {sk})}{P(\\text {my name is Abhi})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text {cs} | \\text {my name is Abhi})=\\frac{P(\\text {my name is Abhi} | \\text {cs}) \\times P(\\text {cs})}{P(\\text {my name is Abhi})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text {en} | \\text {my name is Abhi})=\\frac{P(\\text {my name is Abhi} | \\text {en}) \\times P(\\text {en})}{P(\\text {my name is Abhi})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen Data\n",
    "\n",
    "Since we assume conditional independence across our features, our numerator term for any of the above equations can be broken into the following.\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text {my name is Abhi} | \\text {en}) = P(\\text {my} | \\text {en}) \\times P(\\text {name} | \\text {en}) \\times P(\\text {is} | \\text {en}) \\times P(\\text {Abhi} | \\text {en})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Sentence   \t||   my   \t| is \t| I \t| love \t| name \t| it \t| Abhi \t|\n",
    "|-----------------\t||:------:\t|:--:\t|:-:\t|:----:\t|:----:\t|:--------:\t|:---:\t|\n",
    "| my name is Abhi  \t||    1   \t|  1 \t| 0 \t|   0  \t|   1  \t|     0    \t|  1  \t|\n",
    "| I love it \t||    0   \t|  0 \t| 1 \t|   1  \t|   0  \t|     1    \t|  0  \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each key/value is saved in different lists from the data_preprocessed dictionary by iterating through the dict.\n",
    "sentences_train, y_train=[],[]\n",
    "for k,v in data_preprocessed.items():\n",
    "    for sentence in v:\n",
    "        sentences_train.append(sentence)\n",
    "        y_train.append(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''By using CountVectorizer function we can convert text document to\n",
    "matrix of word count. Matrix which is produced here is sparse matrix.'''\n",
    "\n",
    "vectorizer= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically whats happening here is that the y_train contains the words that are appearing in the dict\n",
    "#while at the same time, using vectorizer, the number of times the words are repeating is the x train.\n",
    "x_train=vectorizer.fit_transform(sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the MultinomialNB from the bayes naive module.\n",
    "#multinomial naive bayes explicitly models the word counts \n",
    "#and adjusts the underlying calculations to deal with in.\n",
    "naive_classifier=MultinomialNB()\n",
    "naive_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Validation Data and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is basically to store the data of each language in a dictionary, while maintaining the fact that\n",
    "the encoding is kept in mind, while at the same time, data_val is used with larger datasets,\n",
    "just so that the validation has much more words to vector upon.\n",
    "'''\n",
    "data_val=dict()\n",
    "data_val['sk']=open_file('Data/Sentences/val_sentences.sk', encoding='utf8')\n",
    "data_val['cs']=open_file('Data/Sentences/val_sentences.csz', encoding='utf8')\n",
    "data_val['en']=open_file('Data/Sentences/val_sentences.en', encoding='utf8')\n",
    "data_val['fr']=open('Data/Sentences/val_sentences.fr', encoding='cp1252').readlines()\n",
    "\n",
    "data_val_preprocessed={k:[preprocess(sentence)for sentence in v] for k,v in data_val.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the same process as training model\n",
    "sentences_val, y_val=[],[]\n",
    "for k,v in data_preprocessed.items():\n",
    "    for sentence in v:\n",
    "        sentences_val.append(sentence)\n",
    "        y_val.append(k)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In scikit-learn estimator api,\n",
    "\n",
    "fit() : used for generating learning model parameters from training data\n",
    "transform() : parameters generated from fit() method,applied upon model to generate transformed data set.\n",
    "fit_transform() : combination of fit() and transform() api on same data set\n",
    "\n",
    "'''\n",
    "x_val=vectorizer.transform(sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the predict functionality is used to make up a prediction about the dataset using \n",
    "the naive bayes thm\n",
    "'''\n",
    "predictions= naive_classifier.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val,predictions,['sk','cs','en','fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_val,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Adjustments and Highlighting Model Shortcomings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The predictions can be improved by playing with the alpha value.\n",
    "In Multinomial Naive Bayes, the alpha parameter is what is known as a hyperparameter; \n",
    "i.e. a parameter that controls the form of the model itself. \n",
    "In most cases, the best way to determine optimal values for hyperparameters is through\n",
    "a grid search over possible parameter values, using cross validation to evaluate the performance\n",
    "of the model on your data at each value.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "naive_classifier=MultinomialNB(alpha=1.0,fit_prior=False)\n",
    "naive_classifier.fit(x_train,y_train)\n",
    "predictions=naive_classifier.predict(x_val)\n",
    "plot_confusion_matrix(y_val,predictions,['sk','cs','en','fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_val,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Subwords to Shift Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy Dataset**\n",
    "\n",
    "playing ; eating ; play ; reads ; tea\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Break each word into characters\n",
    "\n",
    "playing > p l a y i n g\n",
    "\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "Find common character sequences\n",
    "\n",
    "ea, ing, play\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "Convert dataset using these subwords into\n",
    "\n",
    "play ing ; ea t ing ; play ; r ea d s ; t ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The subwords model is much more accurate than conventional classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we improvise and run the same lines of code for the subwords model\n",
    "### and implement our data set to train through Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://arxiv.org/abs/1508.07909\n",
    "# to understand more about subwords in NLP : https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46\n",
    "#its BPE: Byte Pair Encoding\n",
    "\n",
    "import re, collections\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word] \n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        words.extend(sentence.split())\n",
    "        \n",
    "    vocab = defaultdict(int)\n",
    "    for word in words:\n",
    "        vocab[' '.join(word)] += 1\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also taken from original paper\n",
    "for i in range(100):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get) \n",
    "    vocab = merge_vocab(best, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = defaultdict(int)\n",
    "for k, v in vocab.items():\n",
    "    for subword in k.split():\n",
    "        if len(subword) >= 2:\n",
    "            merges[subword] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ordered = sorted(merges, key=merges.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(merge_ordered, open('Data/Auxiliary/merge_ordered.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_subwords(text):\n",
    "    merges = pkl.load(open('Data/Auxiliary/merge_ordered.pkl', 'rb'))\n",
    "    subwords = []\n",
    "    for word in text.split():\n",
    "        for subword in merges:\n",
    "            subword_count = word.count(subword)\n",
    "            if subword_count > 0:\n",
    "                word = word.replace(subword, ' ')\n",
    "                subwords.extend([subword]*subword_count)\n",
    "    return ' '.join(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_subwords('hello my name is abhishek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_subwords={k:[split_into_subwords(sentence)for sentence in v] for k,v in data_preprocessed.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_subwords=[]\n",
    "for sentence in sentences_train:\n",
    "    data_train_subwords.append(split_into_subwords(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_subwords=[]\n",
    "for sentence in sentences_val:\n",
    "    data_val_subwords.append(split_into_subwords(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=vectorizer.fit_transform(data_train_subwords)\n",
    "x_val=vectorizer.transform(data_train_subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_classifier=MultinomialNB(alpha=1,fit_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_classifier.fit(x_train,y_train)\n",
    "predictions=naive_classifier.predict(x_val)\n",
    "plot_confusion_matrix(y_val,predictions,['sk','cs','en','fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_val,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step is to save these models as joblib files in respective folders to access it when we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(naive_classifier,'Data/Models/final_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer,'Data/Vectorizers/final_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we test our code by calling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=joblib.load('Data/Models/final_model.joblib')\n",
    "vectorizer = joblib.load('Data/Vectorizers/final_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text='Hi I am Abhishek'\n",
    "text=preprocess_function(text)\n",
    "text=[split_into_subwords_function(text)]\n",
    "text_vectorized= vectorizer.transform(text)\n",
    "\n",
    "model.predict(text_vectorized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
